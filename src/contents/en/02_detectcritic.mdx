import LinearGraphBlock from '../../components/LinearGraphBlock';

## How can criticism be detected in the media?

We used supervised learning annotation tools to establish a sub-corpus of articles criticizing algorithms and AI.

First of all, manually annotating 2000 press articles deemed ‘critical’ allowed us to construct a machine learning model. Next, by carrying out multiple iterations, including results control phases, the ‘critical’ article detection model was progressively optimized (see the method section). All titles of articles containing arguments or negative semantic elements that were explicitly related to a calculator form (algorithm, AI, robot, etc.) were annotated as critical. Conversely, press articles with neutral, positive, or ambiguous statements or those not directly mentioning a calculator form were annotated as non-critical or were disregarded. For example, titles such as ‘Robocops to replace British bobbies on the streets, police force reveals’ or ‘Growth of AI could boost cybercrime and security threats, report warns’ were annotated as critical, whereas ‘AI to create more than 7m jobs’ or ‘Google so advanced stores will pack your products before you’ve thought of ordering them’ were annotated as non-critical.

<LinearGraphBlock 
  graphType={'histogram'} 
  xVariable={'index'} 
  xLabelVariable={'fullName'} 
  yVariable={'value'} 
  colorVariable={'type'} 
  colorPalette={{"not critic":"#c5c9c9","critic":"#e60c26"}} 
  filtersModeAnd={true} 
  title={'Statistical distribution of critic and non-critic articles rate (2015-2019)'} 
  legend={'Percentage of critic and non-critic articles detected by the model'} 
  normalizeY={true}
/>

Given the work of manually sourcing the initial corpus and the use of the supervised learning annotation method, this corpus makes no claims at being exhaustive or representative. On the contrary, our approach enables an understanding of subjects associated with criticism of AI and algorithms that is as diverse as possible.

Using this categorization protocol, we established a sub-corpus of 2091 press articles categorized as critical. The percentage of such articles was 7.1% on average across the entire corpus, with relatively good stability over the observation period (standard deviation of +/- 2.2%). Research on the analysis of media coverage of AI in Great Britain has shown that the vast majority of publications are on technological advancements and developments in the economic sector, and that the ethical questions associated with the rollout of these technologies remains underrepresented in the press (Brennen et al, 2018).


<div className="methodo-container">
<h2>Method: Use of supervised learning to detect criticism</h2>

**Model used to detect criticism:** fastText - https://fasttext.cc

**Learning corpus:** 2000 articles manually annotated using Prodigy - https://prodi.gy

**Article coding rules:** 
- Critical: critical statements or response to criticism (critical semantic field or arguments), mention of a calculator (algorithm, AI, robot, etc.). Example: ‘Robots put jobs at risk’, ‘Robot lawyers: how humans can fight back’.
- Non-critical: neutral or positive statements, no mention of a calculator. Example: ‘AI to “create more than 7m jobs”’, ‘In the 2020s, artificial intelligence will transform the work of lawyers’.
- Ignored: ambiguous statements, no mention of a calculator. Example: ‘Restaurants are now employing robots – should chefs be worried?’, ‘Need a lawyer? There's an algorithm for that’.


**Comparison of criticism detection models:** 
To ensure the diversity and accuracy of articles, different annotation models were compared and improved through successive iterations, along with the systematic manual control of the results produced by these models. Ultimately, the FastText model produced the most accurate detection of critical articles. 
                  
**Results of the FastText criticism detection model:**                  
 - Precision : 0.94
 - Recall : 0.79
 - F1-score : 0.86
 
This methodological approach allowed us to process a large database that would have been impossible to categorize manually. Although the supervised learning-based automatic categorization tools used do entail a potential risk of not detecting certain critical articles in the corpus, the successive iterations reduce this bias because the results obtained during each iteration show the development of the model’s abilities to detect new critical articles. Furthermore, systematic manual control of all results produced by the algorithm ensure that the sub-corpus of articles categorized as critical do not contain false positives.
</div>
